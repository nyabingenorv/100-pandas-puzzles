{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyabingenorv/100-pandas-puzzles/blob/master/YtSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fquav-n2dRiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YouTube Video to Blog Post Generator**\n",
        "\n",
        "This notebook outlines a comprehensive toolset  for automating the generation of blog posts from YouTube videos. By integrating the YouTube Data API, Gemini AI, and Firebase, the notebook extracts video transcripts, summarizes content, and publishes blog posts to Firestore.\n",
        "\n",
        "**Overview**\n",
        "\n",
        "1. ***YouTube Data Retrieval:***\n",
        "  The notebook utilizes the YouTube Data API to fetch metadata for specific videos or recent uploads from a YouTube channel. This metadata includes details such as the video’s title, description, upload date, view count, and more.\n",
        "\n",
        "2. ***Transcript Extraction:***\n",
        "The YouTube Transcript API is employed to retrieve the transcript of the video. If transcripts are unavailable or disabled, the notebook will skip the processing of that particular video.\n",
        "\n",
        "3. ***Content Summarization:***\n",
        "The transcript is summarized using Gemini AI, which generates a detailed and structured summary of the video content. If Gemini AI considers the content unsafe, the notebook will skip processing that video to ensure only appropriate content is summarized.\n",
        "4. ***Key Points Extraction:***\n",
        "The notebook extracts the most frequent and relevant key points from the video transcript, summarizing the core topics discussed.\n",
        "5. ***Blog Post Creation:***\n",
        "The notebook automatically generates a blog post that includes the video’s summary, key points, and description. The blog post also contains a link to the original YouTube video and a disclaimer regarding the content generation process.\n",
        "6. ***Firestore Integration:***\n",
        "The generated blog post is saved to a Firestore database, along with additional metadata such as the video’s title, author, and thumbnail image.\n",
        "\n"
      ],
      "metadata": {
        "id": "A-qPZtC40XOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wV-3Alj0D9m"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import os\n",
        "import json\n",
        "from googleapiclient.discovery import build\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore\n",
        "import datetime\n",
        "from typing import List, Dict, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        "\n",
        "Next we are going to initialise the following:\n",
        "\n",
        "1.   Google\n",
        "2.   Gemini\n",
        "3.   Firebase\n",
        "4.   Natural language toolkit libraries(nltk)"
      ],
      "metadata": {
        "id": "XRJzvzZm2UZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment variables and initialize Firebase\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"firebase.json\"\n",
        "\n",
        "# Initialize Firebase Firestore\n",
        "db = firestore.client()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Set up API keys\n",
        "GOOGLE_API_KEY = userdata.get('youtube_api')\n",
        "GEMINI_KEY = userdata.get('Gemini')\n",
        "\n",
        "# Initialize YouTube and Gemini API clients\n",
        "youtube = build('youtube', 'v3', developerKey=GOOGLE_API_KEY)\n",
        "genai.configure(api_key=GEMINI_KEY)"
      ],
      "metadata": {
        "id": "HLLE-F3f3LZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i-f8Q6vA0Vky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "***Method to get the channel videos from googles youtube api***\n",
        "\n",
        "Retrieve the most recent videos from a YouTube channel.\n",
        "\n",
        "    Args:\n",
        "        channel_id (str): The ID of the YouTube channel.\n",
        "        video_count (int): The number of recent videos to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, str]]: A list of video information dictionaries.\n"
      ],
      "metadata": {
        "id": "OQM3CpEV4b4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_channel_videos_youtube_api(channel_id: str, video_count: int) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Retrieve the most recent videos from a YouTube channel.\n",
        "\n",
        "    Args:\n",
        "        channel_id (str): The ID of the YouTube channel.\n",
        "        video_count (int): The number of recent videos to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, str]]: A list of video information dictionaries.\n",
        "    \"\"\"\n",
        "    request = youtube.search().list(\n",
        "        part='snippet',\n",
        "        channelId=channel_id,\n",
        "        maxResults=video_count,\n",
        "        order='date'\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    videos = response.get('items', [])\n",
        "    video_infos = []\n",
        "    for video in videos:\n",
        "        video_id = video['id']['videoId']\n",
        "        video_info = get_video_info(video_id)\n",
        "        if video_info:\n",
        "            video_infos.append(video_info)\n",
        "    return video_infos\n"
      ],
      "metadata": {
        "id": "_x0cURz65DPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "***Method to get and parse the youtube video metadata***\n",
        "\n",
        "\n",
        "\n",
        "Retrieve detailed information about a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: A dictionary containing video information or None if the video was not found.\n",
        "\n"
      ],
      "metadata": {
        "id": "O3Vu10A45eA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_info(video_id: str) -> Optional[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Retrieve detailed information about a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: A dictionary containing video information or None if the video was not found.\n",
        "    \"\"\"\n",
        "    request = youtube.videos().list(\n",
        "        part='snippet,contentDetails,statistics',\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if not response['items']:\n",
        "        print(f\"No video found with ID: {video_id}\")\n",
        "        return None\n",
        "\n",
        "    video_info = response['items'][0]\n",
        "    video = {\n",
        "        'id': video_info['id'],\n",
        "        'title': video_info['snippet']['title'],\n",
        "        'description': video_info['snippet'].get('description', 'No description available.'),\n",
        "        'upload_date': video_info['snippet']['publishedAt'],\n",
        "        'uploader': video_info['snippet']['channelTitle'],\n",
        "        'view_count': video_info['statistics'].get('viewCount', 0),\n",
        "        'like_count': video_info['statistics'].get('likeCount', 0),\n",
        "        'duration': video_info['contentDetails']['duration'],\n",
        "        'categories': video_info['snippet'].get('categoryId', []),\n",
        "        'tags': video_info['snippet'].get('tags', []),\n",
        "        'webpage_url': f\"https://www.youtube.com/watch?v={video_id}\"\n",
        "    }\n",
        "    return video"
      ],
      "metadata": {
        "id": "a5FJPZ6C5nuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "***Method to transcribe the youtube video***\n",
        "\n",
        "\n",
        "Retrieve the transcript of a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: The transcript text or None if transcripts are disabled or unavailable.\n",
        "\n"
      ],
      "metadata": {
        "id": "TBr565d1542t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transcript(video_id: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Retrieve the transcript of a YouTube video.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The ID of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: The transcript text or None if transcripts are disabled or unavailable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript_text = ' '.join([entry['text'] for entry in transcript])\n",
        "        if not transcript_text.strip():\n",
        "            print(f\"Transcript is empty for video {video_id}\")\n",
        "            return None\n",
        "        return transcript_text\n",
        "    except TranscriptsDisabled:\n",
        "        print(f\"Transcripts are disabled for video {video_id}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ulqZJSY05-TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***This method leverages the Gemini AI model to\n",
        "provide concise and informative summaries of lengthy texts.***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Summarize the given text using Gemini AI.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be summarized.\n",
        "\n",
        "    Returns:\n",
        "        str: The summarized text.\n",
        "\n"
      ],
      "metadata": {
        "id": "ONT7euKp6ho7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_summarizer(text: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Summarize the given text using Gemini AI, skipping if the content is blocked.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be summarized.\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: The summarized text, or None if the content is blocked.\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    instruction = \"Please provide a detailed summary of the following text. Ensure the summary includes concise summaries of each major section and key points.\"\n",
        "\n",
        "    try:\n",
        "        summary = model.generate_content([instruction, text])\n",
        "        feedback = summary.prompt_feedback.get('safety', {})\n",
        "\n",
        "        if feedback.get('blocked', False):\n",
        "            print(\"Content is considered unsafe by Gemini AI. Skipping this video.\")\n",
        "            return None\n",
        "\n",
        "        print(summary.text)\n",
        "        return summary.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during summarization: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "5qh3rBW76phU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZkNfmVX7ALG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize a text using a transformer-based model\n",
        "def summarize_text(text: str, max_length: int = 250, min_length: int = 0) -> str:\n",
        "    \"\"\"\n",
        "    Summarize a text using a transformer-based summarization model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be summarized.\n",
        "        max_length (int): The maximum length of the summary.\n",
        "        min_length (int): The minimum length of the summary.\n",
        "\n",
        "    Returns:\n",
        "        str: The summary of the text.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Summary not available due to lack of content.\"\n",
        "\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "    try:\n",
        "        summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    except IndexError:\n",
        "        return \"Failed to generate summary due to content length.\"\n",
        "\n",
        "# Function to summarize long texts by splitting them into chunks\n",
        "def summarize_long_text(text: str, chunk_size: int = 1000, max_length: int = 250, min_length: int = 0) -> str:\n",
        "    \"\"\"\n",
        "    Summarize a long text by splitting it into chunks and summarizing each chunk.\n",
        "\n",
        "    Args:\n",
        "        text (str): The long text to be summarized.\n",
        "        chunk_size (int): The maximum size of each chunk in characters.\n",
        "        max_length (int): The maximum length of each summary chunk.\n",
        "        min_length (int): The minimum length of each summary chunk.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated summaries of all text chunks.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"Summary not available due to lack of content.\"\n",
        "\n",
        "    print(f\"Text length before summarization: {len(text)} characters\")\n",
        "    chunks = chunk_text(text, chunk_size)\n",
        "    print(f\"Number of chunks: {len(chunks)}\")\n",
        "\n",
        "    summaries = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing chunk {i+1}/{len(chunks)} with length {len(chunk)} characters\")\n",
        "        summary = summarize_text(chunk, max_length, min_length)\n",
        "        if summary != \"Failed to generate summary due to content length.\":\n",
        "            summaries.append(summary)\n",
        "\n",
        "    return ' '.join(summaries) if summaries else \"No valid content to summarize.\"\n"
      ],
      "metadata": {
        "id": "vYcwdaO98Bw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "***Method to chunk the text if it is longer than expected***\n",
        "\n",
        "Split a large text into smaller chunks of approximately the specified size.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be chunked.\n",
        "        chunk_size (int): The maximum size of each chunk in characters.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks."
      ],
      "metadata": {
        "id": "X-7nwQp98JXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to split a large text into smaller chunks\n",
        "def chunk_text(text: str, chunk_size: int = 1000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split a large text into smaller chunks of approximately the specified size.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to be chunked.\n",
        "        chunk_size (int): The maximum size of each chunk in characters.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of text chunks.\n",
        "    \"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_len = len(sentence)\n",
        "        if current_size + sentence_len <= chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "            current_size += sentence_len\n",
        "        else:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_size = sentence_len\n",
        "\n",
        "    # Add the last chunk if it contains any content\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "B-9cLAVH8Q__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "***Method to process the videos retrieved***\n",
        "\n",
        "    Process videos from a YouTube channel and generate blog posts for each video.\n",
        "\n",
        "    Args:\n",
        "        channel_id (str): The ID of the YouTube channel.\n",
        "        video_count (int): The number of recent videos to process.\n",
        "\n",
        "    Returns:\n",
        "        None"
      ],
      "metadata": {
        "id": "qcNw01NX8Vpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_channel_videos(channel_id: str, video_count: int = 5) -> None:\n",
        "    \"\"\"\n",
        "    Process videos from a YouTube channel and generate blog posts for each video.\n",
        "\n",
        "    Args:\n",
        "        channel_id (str): The ID of the YouTube channel.\n",
        "        video_count (int): The number of recent videos to process.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(f\"Processing channel {channel_id} with video count {video_count}\")\n",
        "\n",
        "    # Fetch recent videos\n",
        "    videos = get_channel_videos_youtube_api(channel_id, video_count)\n",
        "\n",
        "    # Process each video\n",
        "    for video in videos:\n",
        "        print(f\"Processing video: {video['title']}\")\n",
        "\n",
        "        # Retrieve transcript\n",
        "        transcript = get_transcript(video['id'])\n",
        "        if transcript is None:\n",
        "            print(f\"Skipping video {video['id']} due to lack of transcript.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Transcript length: {len(transcript)} characters\")\n",
        "\n",
        "        # Summarize the transcript\n",
        "        summary = gemini_summarizer(transcript)\n",
        "        if summary is None:\n",
        "            print(f\"Skipping video {video['id']} due to unsafe content.\")\n",
        "            continue\n",
        "\n",
        "        key_points = extract_key_points(transcript, num_points=5)\n",
        "\n",
        "        # Create blog content\n",
        "        blog_content = f\"### Summary\\n{summary}\\n\\n### Key Points\\n\" + '\\n'.join([f\"- {point}\" for point in key_points])\n",
        "\n",
        "        # Save blog post to Firebase\n",
        "        save_blog_to_firebase(video, blog_content)\n",
        "\n",
        "    print(\"Finished processing all videos.\")"
      ],
      "metadata": {
        "id": "-jCCzuR28pm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "***Method to save the blog to firebase ***\n",
        "\n",
        "Save the blog post to Firebase Firestore.\n",
        "\n",
        "    Args:\n",
        "        video_info (Dict[str, str]): Information about the video.\n",
        "        blog_content (str): The content of the blog post.\n",
        "\n",
        "    Returns:\n",
        "        None"
      ],
      "metadata": {
        "id": "bx_Vn02p9GZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_blog_to_firebase(video_info: Dict[str, str], blog_content: str) -> None:\n",
        "    \"\"\"\n",
        "    Save the blog post to Firebase Firestore.\n",
        "\n",
        "    Args:\n",
        "        video_info (Dict[str, str]): Information about the video.\n",
        "        blog_content (str): The content of the blog post.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    blog = {\n",
        "        'title': video_info['title'],\n",
        "        'author': video_info['uploader'],\n",
        "        'content': blog_content,\n",
        "        'category': 'YouTube Summary',\n",
        "        'thumbnailUrl': f\"https://img.youtube.com/vi/{video_info['id']}/0.jpg\",  # YouTube thumbnail is normally in this format\n",
        "        'createdAt': datetime.datetime.now().isoformat(),\n",
        "        'videoUrl': video_info['webpage_url'],\n",
        "    }\n",
        "\n",
        "    # Add the blog to Firestore\n",
        "    try:\n",
        "        db.collection('blogs').add(blog)\n",
        "        print(f\"Blog post '{video_info['title']}' saved to Firestore.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while saving to Firestore: {e}\")"
      ],
      "metadata": {
        "id": "x3N4AIIQ9NUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}